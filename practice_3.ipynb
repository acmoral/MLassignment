{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  target  \n",
       "0                            3.92   1065.0       0  \n",
       "1                            3.40   1050.0       0  \n",
       "2                            3.17   1185.0       0  \n",
       "3                            3.45   1480.0       0  \n",
       "4                            2.93    735.0       0  \n",
       "..                            ...      ...     ...  \n",
       "173                          1.74    740.0       2  \n",
       "174                          1.56    750.0       2  \n",
       "175                          1.56    835.0       2  \n",
       "176                          1.62    840.0       2  \n",
       "177                          1.60    560.0       2  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "data = load_wine()\n",
    "df = pd.DataFrame(data=data.data,columns=data.feature_names)\n",
    "df_targets = pd.DataFrame(data=data.target,columns=['target'])\n",
    "df_wine = pd.concat([df,df_targets],axis=1)\n",
    "df_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X train (124, 13)\n",
      "Shape y train (124, 1)\n",
      "Shape X test (54, 13)\n",
      "Shape y test (54, 1)\n"
     ]
    }
   ],
   "source": [
    "#1. split\n",
    "X_train,X_test,y_train,y_test = train_test_split(df,df_targets,test_size=0.3)\n",
    "print(f'Shape X train {X_train.shape}')\n",
    "print(f'Shape y train {y_train.shape}')\n",
    "print(f'Shape X test {X_test.shape}')\n",
    "print(f'Shape y test {y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. decision tree for a single split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "#calculate 1-error ->accuracy \n",
    "predicted = decision_tree.predict(X_test)\n",
    "correct_ones = (y_test['target']==predicted).sum()\n",
    "total_test = y_test.shape[0]\n",
    "mistakes = total_test-correct_ones\n",
    "error = mistakes/total_test\n",
    "accuracy = 1-error\n",
    "print(f'Accuracy : {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that on the $N=54$ trials there are e errors is:\n",
    "$P\\{x\\leq e\\}=\\sum_{j=1}^{e}\\binom{N}{j}p_0^{j}(1-p_0)^{N-j} $\n",
    "\n",
    "Accept if $P< 1-\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P of Confidende Interval 0.6047596483541414\n",
      "Accuracy is 0.8888888888888888, with 95% confidence\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "p0 = error\n",
    "N = total_test\n",
    "probability_confidence_interval = 0\n",
    "alpha=0.05\n",
    "for i in range(1,mistakes+1):\n",
    "    probability_confidence_interval += math.comb(N,i)*(p0**i)*(accuracy**(N-i))\n",
    "\n",
    "print(f'P of Confidende Interval {probability_confidence_interval}')\n",
    "\n",
    "if probability_confidence_interval<1-alpha:\n",
    "    print(f'Accuracy is {accuracy}, with 95% confidence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8955223880597015, 0.8064516129032258, 0.8153846153846154, 0.9230769230769231, 0.9692307692307692, 0.9393939393939394, 0.8181818181818181, 0.8405797101449275, 0.9090909090909091, 0.9384615384615385, 0.8732394366197183, 0.782608695652174, 0.953125, 0.921875, 0.8939393939393939, 0.8382352941176471, 0.8888888888888888, 0.8615384615384616, 0.8472222222222222, 0.9411764705882353, 0.9076923076923077, 0.9090909090909091, 0.9402985074626866, 0.8857142857142857, 0.9117647058823529, 0.873015873015873, 0.8461538461538461, 0.9411764705882353, 0.921875, 0.875]\n"
     ]
    }
   ],
   "source": [
    "#2 boostrap\n",
    "df_train = pd.concat([X_train,y_train],axis=1)\n",
    "accuracies =[]\n",
    "N_train = y_train.shape[0]\n",
    "# x_t = []\n",
    "for i in range(30):\n",
    "    boost_train = df_train.sample(n=int(N_train*0.63),replace=True)\n",
    "    boost_val = df_train[~df_train.index.isin(boost_train.index)]\n",
    "    decision_tree = DecisionTreeClassifier()\n",
    "    decision_tree.fit(boost_train[data.feature_names],boost_train[['target']])\n",
    "    predicted = decision_tree.predict(boost_val[data.feature_names])\n",
    "    correct_ones = (boost_val['target']==predicted).sum()\n",
    "    # x_t.append(((boost_val['target']==predicted)*1).to_list())\n",
    "    total_test = boost_val.shape[0]\n",
    "    mistakes = total_test-correct_ones\n",
    "    error = mistakes/total_test\n",
    "    accuracy = 1-error\n",
    "    accuracies.append(accuracy)\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interval -> [0.8898,0.8881]\n",
      "Mean is in this interval with 95% confidence\n"
     ]
    }
   ],
   "source": [
    "#t test\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "t_critical_value = scipy.stats.t.ppf(0.025, 29)\n",
    "m = sum(accuracies)/len(accuracies)\n",
    "S = sum(np.array(accuracies-m)**2)/(len(accuracies)-1)\n",
    "interval_left = m - t_critical_value*S/math.sqrt(30)\n",
    "interval_right = m + t_critical_value*S/math.sqrt(30)\n",
    "print('interval -> [{:.4f},{:.4f}]'.format(interval_left,interval_right))\n",
    "print('Mean is in this interval with 95% confidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. same t test but with RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df_train = pd.concat([X_train,y_train],axis=1)\n",
    "accuracies =[]\n",
    "N_train = y_train.shape[0]\n",
    "for i in range(30):\n",
    "    boost_train = df_train.sample(n=int(N_train*0.63),replace=True)\n",
    "    boost_val = df_train[~df_train.index.isin(boost_train.index)]\n",
    "    decision_tree = RandomForestClassifier()\n",
    "    decision_tree.fit(boost_train[data.feature_names],boost_train[['target']])\n",
    "    predicted = decision_tree.predict(boost_val[data.feature_names])\n",
    "    correct_ones = (boost_val['target']==predicted).sum()\n",
    "    total_test = boost_val.shape[0]\n",
    "    mistakes = total_test-correct_ones\n",
    "    error = mistakes/total_test\n",
    "    accuracy = 1-error\n",
    "    accuracies.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9552238805970149, 0.9705882352941176, 0.9402985074626866, 0.9848484848484849, 0.9836065573770492, 0.9375, 0.9857142857142858, 0.9841269841269842, 0.9696969696969697, 1.0, 0.953125, 0.9545454545454546, 0.984375, 0.9130434782608696, 0.9411764705882353, 0.984375, 1.0, 0.9841269841269842, 0.9696969696969697, 1.0, 0.9393939393939394, 0.9857142857142858, 0.9076923076923077, 0.9722222222222222, 0.9855072463768116, 0.9855072463768116, 0.9402985074626866, 0.9710144927536232, 0.9545454545454546, 0.9696969696969697]\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interval -> [0.9671,0.9667]\n",
      "Mean is in this interval with 95% confidence\n"
     ]
    }
   ],
   "source": [
    "#t test\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "t_critical_value = scipy.stats.t.ppf(0.025, 29)\n",
    "m = sum(accuracies)/len(accuracies)\n",
    "S = sum(np.array(accuracies-m)**2)/(len(accuracies)-1)\n",
    "interval_left = m - t_critical_value*S/math.sqrt(30)\n",
    "interval_right = m + t_critical_value*S/math.sqrt(30)\n",
    "print('interval -> [{:.4f},{:.4f}]'.format(interval_left,interval_right))\n",
    "print('Mean is in this interval with 95% confidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value : 6.125\n",
      "chi square critical value: 0.003932140000019522\n",
      "Hypothesis rejected\n",
      "The Error is significantly different in both classifiers\n"
     ]
    }
   ],
   "source": [
    "#4. McNemar's test only one split needed\n",
    "\n",
    "#classifier 1\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train,y_train)\n",
    "\n",
    "predicted_tree = decision_tree.predict(X_test)\n",
    "\n",
    "# classifier 2\n",
    "decision_trees = RandomForestClassifier()\n",
    "decision_trees.fit(X_train,y_train.to_numpy().ravel())\n",
    "\n",
    "predicted_trees = decision_trees.predict(X_test)\n",
    "\n",
    "e_11 = ((y_test['target']==predicted_tree)&(y_test['target']==predicted_trees )).sum()\n",
    "e_00 = ((y_test['target']!=predicted_tree)&(y_test['target']!=predicted_trees )).sum()\n",
    "e_01 = ((y_test['target']!=predicted_tree)&(y_test['target']==predicted_trees )).sum()\n",
    "e_10 = ((y_test['target']==predicted_tree)&(y_test['target']!=predicted_trees )).sum()\n",
    "value = (abs(e_01-e_10)-1)**2/(e_01+e_10)\n",
    "xi2 = scipy.stats.chi2.ppf(0.05, df=1)\n",
    "print(f'value : {value}')\n",
    "print(f'chi square critical value: {xi2}')\n",
    "if value<xi2:\n",
    "    print('Hypothesis accepted')\n",
    "else:\n",
    "    print('Hypothesis rejected')  \n",
    "    print('The Error is significantly different in both classifiers') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
