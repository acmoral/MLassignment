{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extraer_notas import jams_a_vec,scale_modes   \n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar nombres de archivos\n",
    "files = [file for file in os.listdir('./data/annotation') if file.endswith('.jams')]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 14, 53, 12, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convertir a one hot\n",
    "pitch_vecs, dur_vecs, offset_vecs, chord_vecs,possible_scales = jams_a_vec('./data/annotation/'+files[0])\n",
    "len(pitch_vecs[0]), len(dur_vecs[0]), len(offset_vecs[0]), len(chord_vecs[0]),len(possible_scales[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_entropies(file_name,show_plot=False,save_plot=None,generated=False):\n",
    "    if generated:\n",
    "        gen_np = np.load('./data/generated/'+file_name)\n",
    "        pitch_vecs = gen_np[:,0:128]\n",
    "        dur_vecs = gen_np[:,128:142]\n",
    "        offset_vecs = gen_np[:,142:195]\n",
    "\n",
    "    # convertir a one hot\n",
    "    else:\n",
    "        pitch_vecs, dur_vecs, offset_vecs, _,_ = jams_a_vec('./data/annotation/'+file_name)\n",
    "    #count notes \n",
    "    # print(len(pitch_vecs[0]), len(dur_vecs[0]), len(offset_vecs[0]))\n",
    "    count_pitch  = np.array(pitch_vecs).sum(axis=0)\n",
    "    \n",
    "    #calculate entropy pitch\n",
    "    count_pitch_normalized = count_pitch/sum(count_pitch)\n",
    "    entropy_pitch = -sum(count_pitch_normalized*np.log2(count_pitch_normalized,where=count_pitch_normalized!=0,out=np.zeros_like(count_pitch_normalized)))\n",
    "    #lo mismo para la duraci√≥n\n",
    "    count_dur  = np.array(dur_vecs).sum(axis=0)\n",
    "    count_dur_normalized = count_dur/sum(count_dur)\n",
    "    entropy_dur = -sum(count_dur_normalized*np.log2(count_dur_normalized,where=count_dur_normalized!=0,out=np.zeros_like(count_dur_normalized)))\n",
    "    # lo mismo para el offset\n",
    "    count_offset  = np.array(offset_vecs).sum(axis=0)\n",
    "    count_offset_normalized = count_offset/sum(count_offset)\n",
    "    entropy_offset = -sum(count_offset_normalized*np.log2(count_offset_normalized,where=count_offset_normalized!=0,out=np.zeros_like(count_offset_normalized)))\n",
    "    if show_plot or save_plot:\n",
    "        fig, axs = plt.subplots(3, 1, figsize=(8, 10))\n",
    "        axs[0].bar(list(range(1,len(count_pitch)+1)), count_pitch)\n",
    "        axs[0].set_xlabel('Note')\n",
    "        axs[1].bar(list(range(len(count_dur))), count_dur)\n",
    "        axs[1].set_xlabel('Duration')\n",
    "        axs[2].bar(list(range(len(count_offset))), count_offset)\n",
    "        axs[2].set_xlabel('Offset')\n",
    "        fig.text(0.06, 0.5, 'Counts', ha='center', va='center', rotation='vertical')\n",
    "        if save_plot:\n",
    "            plt.savefig(save_plot)\n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "    return entropy_pitch,entropy_dur,entropy_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy_pitch       3.556122\n",
      "entropy_duration    2.799534\n",
      "entropy_offset      2.895642\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [ ]\n",
    "for file in files:\n",
    "    pitch,duration,offset = calculate_entropies(file,show_plot=False,save_plot='./data/metrics/plots/'+file[:-5]+'.png')\n",
    "    data.append({'file': file, 'entropy_pitch': pitch, 'entropy_duration': duration, 'entropy_offset': offset})\n",
    "    \n",
    "entropies = pd.DataFrame(data)\n",
    "\n",
    "entropies.to_csv('./data/metrics/entropies.csv', index=False)\n",
    "\n",
    "print(entropies[['entropy_pitch', 'entropy_duration', 'entropy_offset']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy_pitch       0.439438\n",
      "entropy_duration    0.308698\n",
      "entropy_offset      0.603527\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "entropies = pd.read_csv('./data/metrics/entropies.csv')\n",
    "#calculate std\n",
    "print(entropies[['entropy_pitch', 'entropy_duration', 'entropy_offset']].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_name(vect):\n",
    "    octava_separada = [vect[i:i +12] for i in range(0, len(vect),12)]\n",
    "    octava_separada[-1] = np.insert(octava_separada[-1],-1,[0,0,0,0])[:12]\n",
    "    note_any_octave = np.array(np.ufunc.reduce(np.logical_or, octava_separada)).astype(int)\n",
    "    return note_any_octave\n",
    "\n",
    "#check harmonic coherence\n",
    "def harmonic_coherence(pitch_vecs,possible_scales):\n",
    "    matches = []\n",
    "    for i in range(len(pitch_vecs)):\n",
    "        note = midi_to_name(pitch_vecs[i])\n",
    "        this_note_possible_scales = np.array(possible_scales[i])\n",
    "        match = np.dot(this_note_possible_scales,note).sum()/len(this_note_possible_scales)\n",
    "        matches.append(match)\n",
    "    percentage = np.array(matches).mean()\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.864992465757785\n"
     ]
    }
   ],
   "source": [
    "harmonic_coherences = []\n",
    "for file in files:\n",
    "    pitch_vecs, dur_vecs, offset_vecs, chord_vecs,possible_scales = jams_a_vec('./data/annotation/'+file)\n",
    "    harmonic_coherences.append(harmonic_coherence(pitch_vecs,possible_scales))\n",
    "hc = pd.DataFrame({'file': files, 'harmonic_coherence': harmonic_coherences})\n",
    "hc.to_csv('./data/metrics/harmonic_coherence.csv', index=False)\n",
    "print(hc['harmonic_coherence'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08184772531588434\n"
     ]
    }
   ],
   "source": [
    "hc = pd.read_csv('./data/metrics/harmonic_coherence.csv')\n",
    "print(hc['harmonic_coherence'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform all jam files to vec\n",
    "all_file_list = []\n",
    "for file in files:\n",
    "    pitch_vecs, dur_vecs, offset_vecs, chord_vecs,possible_scales = jams_a_vec('./data/annotation/'+file)\n",
    "    all_file_list.append([pitch_vecs, dur_vecs, offset_vecs, chord_vecs,possible_scales])\n",
    "#save all files\n",
    "import pickle\n",
    "with open('./data/metrics/all_files.pkl', 'wb') as f:\n",
    "    pickle.dump(all_file_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_list = pickle.load(open('./data/metrics/all_files.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_distances(pitch_vecs):\n",
    "    distances = []\n",
    "    for i in range(0,len(pitch_vecs)-1):\n",
    "        note_1 = pitch_vecs[i]\n",
    "        note_2 = pitch_vecs[i+1]\n",
    "        distance = abs(np.where(note_1==1)[0]  - np.where(note_2==1)[0])\n",
    "        distances.append(distance[0])\n",
    "    distances.append(0)\n",
    "    return distances\n",
    "# relative_distances_ = relative_distances(pitch_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extraer_notas import longest_contiguous_common_subsequence\n",
    "def common_subsequence(jams1,jams2):\n",
    "    pitch_vecs1, dur_vecs1, _,_,_= jams1\n",
    "    pitch_vecs2, dur_vecs2, _,_,_= jams2\n",
    "    relative_distances1 = relative_distances(pitch_vecs1)\n",
    "    relative_distances2 = relative_distances(pitch_vecs2)\n",
    "    array1 = [[relative_distances1[i],dur_vecs1[i].astype(int).tolist()] for i in range(len(relative_distances1))]\n",
    "    array2 = [[relative_distances2[i],dur_vecs2[i].astype(int).tolist()] for i in range(len(relative_distances2))]\n",
    "    if longest_contiguous_common_subsequence(array1,array2):\n",
    "        return len(longest_contiguous_common_subsequence(array1,array2))\n",
    "    else:\n",
    "        return 0\n",
    "# common_subsequence(all_file_list[1],all_file_list[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros((len(all_file_list),len(all_file_list)))\n",
    "for i in range(len(all_file_list)):\n",
    "    matrix[i,i] = 0\n",
    "    for j in range(i+1,len(all_file_list)):\n",
    "        matrix[i,j] = common_subsequence(all_file_list[i],all_file_list[j])\n",
    "        matrix[j,i] = matrix[i,j]\n",
    "        # print(files[i],files[j],common_subsequence(all_file_list[i],all_file_list[j]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/metrics/common_subsequence.npy', matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_subsequence_file = np.load('./data/metrics/common_subsequence.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average common subsequence per file\n",
    "average_common_subsequence = common_subsequence_file.mean(axis=1)\n",
    "acs = pd.DataFrame({'file': files, 'average_common_subsequence': average_common_subsequence,'std': common_subsequence_file.std(axis=1)})\n",
    "acs.to_csv('./data/metrics/average_common_subsequence.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>average_common_subsequence</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>01_Rock2-85-F_comp.jams</td>\n",
       "      <td>2.038889</td>\n",
       "      <td>0.944951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>00_SS2-107-Ab_comp.jams</td>\n",
       "      <td>1.788889</td>\n",
       "      <td>0.557663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>04_Rock3-117-Bb_solo.jams</td>\n",
       "      <td>1.955556</td>\n",
       "      <td>0.580443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>02_SS2-107-Ab_comp.jams</td>\n",
       "      <td>2.325000</td>\n",
       "      <td>0.990195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>04_BN1-147-Gb_solo.jams</td>\n",
       "      <td>1.780556</td>\n",
       "      <td>0.546870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file  average_common_subsequence       std\n",
       "102    01_Rock2-85-F_comp.jams                    2.038889  0.944951\n",
       "52     00_SS2-107-Ab_comp.jams                    1.788889  0.557663\n",
       "285  04_Rock3-117-Bb_solo.jams                    1.955556  0.580443\n",
       "172    02_SS2-107-Ab_comp.jams                    2.325000  0.990195\n",
       "243    04_BN1-147-Gb_solo.jams                    1.780556  0.546870"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acs.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6824011947579548"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acs['std'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.884151234567901\n",
      "0.21893433874155213\n"
     ]
    }
   ],
   "source": [
    "acs = pd.read_csv('./data/metrics/average_common_subsequence.csv')\n",
    "print(acs['average_common_subsequence'].mean())\n",
    "print(acs['average_common_subsequence'].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only_solos = [file for file in files if 'solo' in file]\n",
    "# matrix = np.zeros((len(only_solos),len(only_solos)))\n",
    "# for i in range(len(only_solos)):\n",
    "#    for j in range(i+1,len(only_solos)):\n",
    "#        matrix[i][j] = common_subsequence('./data/annotation/'+only_solos[i],'./data/annotation/'+only_solos[j])\n",
    "\n",
    "# matrix.save('./data/metrics/common_subsequence.npy')\n",
    "jams1 = jams_a_vec('./data/annotation/'+files[0])\n",
    "jams2 = jams_a_vec('./data/annotation/'+files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_subsequence(jams1,jams2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_files = [file for file in os.listdir('./data/generated') if file.endswith('.npy')]\n",
    "len(generated_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_file = np.load('./data/generated/'+generated_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 219)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_vecs, dur_vecs, offset_vecs, chord_vecs,next_chord = gen_file[:,:128],gen_file[:,128:142],gen_file[:,142:195],gen_file[:,195:207],gen_file[:,207:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 128), (120, 14), (120, 53), (120, 12), (120, 12))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitch_vecs.shape, dur_vecs.shape, offset_vecs.shape, chord_vecs.shape,next_chord.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy_pitch       3.278294\n",
      "entropy_duration    2.867671\n",
      "entropy_offset      3.129005\n",
      "dtype: float64\n",
      "entropy_pitch       0.436090\n",
      "entropy_duration    0.242717\n",
      "entropy_offset      0.529673\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for file in generated_files:\n",
    "    pitch,duration,offset=calculate_entropies(file,show_plot=False,save_plot='./data/metrics/generated/plots/'+file[:-4]+'.png',generated=True)\n",
    "    data.append({'file': file, 'entropy_pitch': pitch, 'entropy_duration': duration, 'entropy_offset': offset})\n",
    "data = pd.DataFrame(data)\n",
    "data.to_csv('./data/metrics/generated/entropies.csv', index=False)\n",
    "print(data[['entropy_pitch', 'entropy_duration', 'entropy_offset']].mean())\n",
    "print(data[['entropy_pitch', 'entropy_duration', 'entropy_offset']].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_scales_from_vec(vec):\n",
    "    chord = np.where(vec == 1)[0]\n",
    "    type_of_chord = chord-chord[0]\n",
    "    possible_scales = []\n",
    "    for key in scale_modes.keys():\n",
    "        if set(type_of_chord) <= set(scale_modes[key]):\n",
    "            possible_scales.append(key)\n",
    "    possible_scale_vecs = []\n",
    "    for scale in possible_scales:\n",
    "        scale_vec = np.zeros(12)\n",
    "        scale_idx = scale_modes[scale]\n",
    "        for idx in scale_idx:\n",
    "            scale_vec[(chord[0] + idx) % 12] = 1\n",
    "        possible_scale_vecs.append(scale_vec)\n",
    "    return possible_scale_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8615361111111111\n",
      "0.053794238313759735\n"
     ]
    }
   ],
   "source": [
    "# coherencia \n",
    "harmonic_coherences = []\n",
    "for file in generated_files:\n",
    "    gen_np = np.load('./data/generated/'+file)\n",
    "    pitch_vecs, dur_vecs, offset_vecs, chord_vecs,next_chord_vec = gen_np[:,:128],gen_np[:,128:142],gen_np[:,142:195],gen_np[:,195:207],gen_np[:,207:]\n",
    "    possible_scales =[possible_scales_from_vec(vec) for vec in chord_vecs]\n",
    "    harmonic_coherences.append(harmonic_coherence(pitch_vecs,possible_scales))\n",
    "    hc = pd.DataFrame({'file': file, 'harmonic_coherence': harmonic_coherences})\n",
    "hc.to_csv('./data/metrics/harmonic_coherence.csv', index=False)\n",
    "print(hc['harmonic_coherence'].mean())\n",
    "print(hc['harmonic_coherence'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.97777778 1.95       2.01666667 1.875      1.91388889 1.90833333\n",
      " 2.03333333 1.98611111 2.10555556 2.01111111 2.13055556 2.06388889\n",
      " 2.14722222 2.00277778 1.92777778 2.00555556 1.88333333 1.82777778\n",
      " 2.06388889 1.975      1.94166667 1.94166667 1.95277778 2.10833333\n",
      " 1.98055556 1.70277778 1.83333333 1.71944444 2.03055556 2.08888889\n",
      " 1.86666667 1.94166667 2.00833333 1.96666667 1.99722222 1.95555556\n",
      " 1.96111111 2.03055556 2.15833333 2.03333333 2.21666667 1.97777778\n",
      " 1.97222222 1.92777778 2.06666667 1.83333333 2.00277778 2.03055556\n",
      " 1.75555556 2.04444444 2.03055556 2.00833333 1.89444444 1.875\n",
      " 1.825      2.05277778 1.95277778 1.95555556 1.71666667 1.72777778\n",
      " 2.1        2.06388889 2.04722222 2.12222222 1.725      1.87222222\n",
      " 2.08611111 1.92777778 2.06111111 1.93055556 1.94444444 1.88611111\n",
      " 1.69166667 1.925      2.07222222 1.94444444 1.98888889 2.01944444\n",
      " 1.93611111 2.04722222 1.93611111 2.10555556 2.         2.12777778\n",
      " 1.88055556 2.06944444 2.10555556 1.96111111 2.00277778 1.98333333\n",
      " 1.97777778 1.94444444 2.04166667 1.85555556 1.76666667 2.00555556\n",
      " 2.03611111 2.01111111 1.94166667 1.91666667]\n",
      "[0.49950593 0.57033129 0.62338681 0.66536331 0.51728607 0.7226014\n",
      " 0.57154761 0.56502349 0.75643331 0.61903948 0.53454001 0.59519034\n",
      " 0.74758356 0.70906281 0.53779959 0.54769438 0.63486657 0.57571748\n",
      " 0.67398186 0.64285604 0.65356926 0.64930518 0.58261199 0.61638507\n",
      " 0.39743584 0.60368595 0.7490735  0.70447752 0.6554555  0.49203533\n",
      " 0.63595947 0.51579227 0.57487921 0.55677644 0.60322564 0.51985278\n",
      " 0.50950836 0.66804834 0.65399414 0.57638722 0.64785972 0.49391357\n",
      " 0.66190429 0.5429401  0.56862407 0.53229065 0.66038125 0.73911337\n",
      " 0.50686643 0.62647357 0.63391177 0.6561144  0.66664352 0.73668741\n",
      " 0.68713374 0.69120913 0.66707742 0.45731368 0.7247605  0.49811373\n",
      " 0.64204534 0.49869428 0.62848412 0.73147914 0.61412766 0.57764422\n",
      " 0.58767564 0.59189484 0.62060803 0.55593737 0.67677521 0.59705424\n",
      " 0.59808723 0.49714462 0.74184871 0.66434782 0.39424878 0.52404381\n",
      " 0.66150199 0.58736044 0.59050486 0.5868676  0.54262735 0.57764422\n",
      " 0.6052178  0.64821098 0.70708496 0.6443247  0.54005458 0.6664583\n",
      " 0.65790539 0.63440455 0.58303564 0.54387726 0.66749948 0.65402363\n",
      " 0.7426128  0.60082248 0.59107952 0.62694143]\n"
     ]
    }
   ],
   "source": [
    "# common subsecuence\n",
    "matrix = np.zeros((len(generated_files),len(all_file_list)))\n",
    "for i in range(len(generated_files)):\n",
    "    file_1 = np.load('./data/generated/'+generated_files[i])\n",
    "    for j in range(len(all_file_list)):\n",
    "        file_2 = all_file_list[j]\n",
    "        matrix[i,j] = common_subsequence([file_1[:,:128],file_1[:,128:142],file_1[:,142:195],file_1[:,195:207],file_1[:,207:]],file_2)\n",
    "np.save('./data/metrics/generated/common_subsequence.npy', matrix)\n",
    "print(matrix.mean(axis=1))\n",
    "print(matrix.std(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9695277777777775\n",
      "0.10623571436279251\n"
     ]
    }
   ],
   "source": [
    "print(matrix.mean(axis=1).mean())\n",
    "print(matrix.mean(axis=1).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(all_file_list[0][0]).sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# divergencia de kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.240699024927538\n",
      "1.3822571816779348\n",
      "3.3342948217464157\n",
      "1.3702818670695018\n",
      "0.7182674569193387\n",
      "1.3433411903153627\n"
     ]
    }
   ],
   "source": [
    "# calculate the kl divergence\n",
    "def kl_divergence(P, Q):\n",
    "    epsilon = 0.00001\n",
    "    # Convert inputs to numpy arrays\n",
    "    P = np.asarray(P, dtype=np.float64)\n",
    "    Q = np.asarray(Q, dtype=np.float64)\n",
    "    P = P+epsilon\n",
    "    Q = Q+epsilon\n",
    "    # Compute the KL divergence\n",
    "    kl_div = np.sum(np.where(P != 0, P * np.log2(P / Q), 0))\n",
    "    \n",
    "    return kl_div\n",
    "kl_divergences_pitch = np.zeros((len(generated_files),len(all_file_list)))\n",
    "kl_divergences_dur = np.zeros((len(generated_files),len(all_file_list)))\n",
    "kl_divergences_offset = np.zeros((len(generated_files),len(all_file_list)))\n",
    "\n",
    "for i in range(len(generated_files)):\n",
    "    gen_np = np.load('./data/generated/'+generated_files[i])\n",
    "    pitch_vecs, dur_vecs, offset_vecs, _,__dict__ = gen_np[:,:128],gen_np[:,128:142],gen_np[:,142:195],gen_np[:,195:207],gen_np[:,207:]\n",
    "    pitch_vecs_counts_normalized = np.array(pitch_vecs).sum(axis=0)/sum(np.array(pitch_vecs).sum(axis=0))\n",
    "    dur_vecs_counts_normalized = np.array(dur_vecs).sum(axis=0)/sum(np.array(dur_vecs).sum(axis=0))\n",
    "    offset_vecs_counts_normalized = np.array(offset_vecs).sum(axis=0)/sum(np.array(offset_vecs).sum(axis=0))\n",
    "    for j in range(len(all_file_list)):\n",
    "        pitch_vecs_org, dur_vecs_org, offset_vecs_org, _,_ = all_file_list[j]\n",
    "        pitch_vecs_org_counts_normalized = np.array(pitch_vecs_org).sum(axis=0)/sum(np.array(pitch_vecs_org).sum(axis=0))\n",
    "        dur_vecs_org_counts_normalized = np.array(dur_vecs_org).sum(axis=0)/sum(np.array(dur_vecs_org).sum(axis=0))\n",
    "        offset_vecs_org_counts_normalized = np.array(offset_vecs_org).sum(axis=0)/sum(np.array(offset_vecs_org).sum(axis=0))\n",
    "        kl_divergences_pitch[i,j] = kl_divergence(pitch_vecs_org_counts_normalized,pitch_vecs_counts_normalized)\n",
    "        kl_divergences_dur[i,j] = kl_divergence(dur_vecs_org_counts_normalized,dur_vecs_counts_normalized)\n",
    "        kl_divergences_offset[i,j] = kl_divergence(offset_vecs_org_counts_normalized,offset_vecs_counts_normalized)\n",
    "        \n",
    "# print(kl_divergences_pitch.mean(axis=1))\n",
    "# print(kl_divergences_pitch.std(axis=1))\n",
    "\n",
    "# print(kl_divergences_dur.mean(axis=1))\n",
    "# print(kl_divergences_dur.std(axis=1))\n",
    "\n",
    "# print(kl_divergences_offset.mean(axis=1))\n",
    "# print(kl_divergences_offset.std(axis=1))\n",
    "\n",
    "print(kl_divergences_pitch.mean(axis=1).mean())\n",
    "print(kl_divergences_dur.mean(axis=1).mean())\n",
    "print(kl_divergences_offset.mean(axis=1).mean())\n",
    "print(kl_divergences_pitch.mean(axis=1).std())\n",
    "print(kl_divergences_dur.mean(axis=1).std())\n",
    "print(kl_divergences_offset.mean(axis=1).std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/metrics/generated/kl_divergences_pitch.npy', kl_divergences_pitch)\n",
    "np.save('./data/metrics/generated/kl_divergences_dur.npy', kl_divergences_dur)\n",
    "np.save('./data/metrics/generated/kl_divergences_offset.npy', kl_divergences_offset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projfinal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
